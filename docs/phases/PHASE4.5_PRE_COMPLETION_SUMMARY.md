# Phase 4.5 Pre-Completion Summary
## Quality Assurance & AgentOS Integration

**Status:** Planning Phase
**Target Completion:** 1-2 weeks
**Dependencies:** Phase 4 Complete ‚úÖ
**Next Phase:** Phase 5 (Multi-Tenant SaaS Platform)

---

## üéØ Phase 4.5 Objectives

**Bridge the gap between local MVP and production SaaS by:**

1. **Quality Assurance** - Comprehensive testing of Phases 1-4
2. **AgentOS Integration** - Deploy workflow as a RESTful API service
3. **MCP Server** - Enable Model Context Protocol for easy Claude integration
4. **Performance Optimization** - Identify and fix bottlenecks
5. **Error Handling** - Robust failure recovery and retry logic
6. **Production Readiness** - Logging, monitoring, and deployment prep

**Why Phase 4.5?**
- Validate the MVP works reliably before building SaaS infrastructure
- Get AgentOS running locally to understand deployment patterns
- Enable MCP for testing via Claude Desktop/Code
- Catch bugs and edge cases before production
- Establish baseline performance metrics

---

## üèóÔ∏è Architecture Evolution

### Current State (Phase 4)
```
Local Python Script
‚îú‚îÄ‚îÄ test_phase4.py (manual execution)
‚îú‚îÄ‚îÄ workflow.py (phase1_2_3_4_workflow)
‚îî‚îÄ‚îÄ Output: JSON files in local directory

Limitations:
‚ùå No API access
‚ùå Manual execution only
‚ùå No error recovery
‚ùå No session management
‚ùå No production monitoring
```

### Target State (Phase 4.5)
```
AgentOS Instance (Local)
‚îú‚îÄ‚îÄ my_octave_os.py (FastAPI app)
‚îÇ   ‚îú‚îÄ‚îÄ Endpoints: /agents, /workflows, /sessions, /config
‚îÇ   ‚îú‚îÄ‚îÄ MCP Server: /mcp (Claude Desktop integration)
‚îÇ   ‚îî‚îÄ‚îÄ API Docs: /docs (Swagger UI)
‚îú‚îÄ‚îÄ Workflow Runs via API
‚îÇ   ‚îú‚îÄ‚îÄ curl http://localhost:7777/workflows/sales-playbook/runs
‚îÇ   ‚îú‚îÄ‚îÄ Session management
‚îÇ   ‚îî‚îÄ‚îÄ Streaming responses
‚îú‚îÄ‚îÄ Quality Assurance
‚îÇ   ‚îú‚îÄ‚îÄ Edge case testing (10+ scenarios)
‚îÇ   ‚îú‚îÄ‚îÄ Performance benchmarks
‚îÇ   ‚îú‚îÄ‚îÄ Error recovery tests
‚îÇ   ‚îî‚îÄ‚îÄ Data validation tests
‚îî‚îÄ‚îÄ Monitoring & Logging
    ‚îú‚îÄ‚îÄ Structured logging
    ‚îú‚îÄ‚îÄ Performance metrics
    ‚îî‚îÄ‚îÄ Error tracking

Benefits:
‚úÖ RESTful API access
‚úÖ Session management
‚úÖ Streaming responses
‚úÖ MCP integration (use from Claude Desktop!)
‚úÖ Interactive API docs
‚úÖ Production-ready logging
‚úÖ Deployment blueprint for Phase 5
```

---

## üìã Phase 4.5 Task Breakdown

### 4.5.1 - AgentOS Setup & Integration
**Goal:** Deploy the workflow to AgentOS with full API access

#### Task 1: Create AgentOS Instance

Create `my_octave_os.py`:

```python
"""
Octave Clone AgentOS - Sales Intelligence Platform
Exposes the phase1_2_3_4_workflow as a RESTful API with MCP support.
"""

from agno.os import AgentOS
from agno.db.postgres import PostgresDb
from workflow import phase1_2_3_4_workflow
import os

# Optional: Setup database for session persistence
# For Phase 4.5, we'll start without DB (in-memory sessions)
# For Phase 5, we'll add PostgresDb
db = None  # PostgresDb(db_url=os.getenv("DATABASE_URL")) if needed

# Create AgentOS instance
agent_os = AgentOS(
    id="octave-clone-os",
    description="Sales Intelligence Platform - Generate AI-powered playbooks from vendor and prospect websites",

    # Register our workflow
    workflows=[phase1_2_3_4_workflow],

    # Enable MCP for Claude Desktop integration
    enable_mcp_server=True,

    # Optional database
    db=db,
)

# Get FastAPI app
app = agent_os.get_app()

if __name__ == "__main__":
    print("=" * 80)
    print("üöÄ OCTAVE CLONE AgentOS")
    print("=" * 80)
    print("\nüìç Endpoints:")
    print("   ‚Ä¢ App Interface:  http://localhost:7777")
    print("   ‚Ä¢ API Docs:       http://localhost:7777/docs")
    print("   ‚Ä¢ Configuration:  http://localhost:7777/config")
    print("   ‚Ä¢ MCP Server:     http://localhost:7777/mcp")
    print("\nüîß API Usage:")
    print("   curl -X POST http://localhost:7777/workflows/phase1_2_3_4_workflow/runs \\")
    print("        -H 'Content-Type: application/x-www-form-urlencoded' \\")
    print("        -d 'vendor_domain=https://octavehq.com' \\")
    print("        -d 'prospect_domain=https://sendoso.com'")
    print("\n" + "=" * 80)

    agent_os.serve(app="my_octave_os:app", reload=True)
```

**Testing the AgentOS API:**

```bash
# Start the server
python my_octave_os.py

# Test workflow execution via API
curl -X POST http://localhost:7777/workflows/phase1_2_3_4_workflow/runs \
    -H 'Content-Type: application/x-www-form-urlencoded' \
    -d 'vendor_domain=https://octavehq.com' \
    -d 'prospect_domain=https://sendoso.com' \
    -d 'stream=true'

# Check configuration
curl http://localhost:7777/config

# View API docs
open http://localhost:7777/docs
```

**Tasks:**
- [ ] Create `my_octave_os.py` with AgentOS setup
- [ ] Test workflow execution via API
- [ ] Verify streaming responses work
- [ ] Test session management (create, retrieve, delete)
- [ ] Document all API endpoints
- [ ] Create example cURL commands

**Deliverables:**
- `my_octave_os.py` - AgentOS instance
- API accessible at `http://localhost:7777`
- Working `/workflows/{workflow_id}/runs` endpoint
- Documentation of API usage

---

#### Task 2: MCP Server Integration

**Goal:** Enable Claude Desktop/Code to interact with the workflow via MCP

**MCP Capabilities:**
- Run workflows from Claude Desktop
- Manage sessions
- Retrieve playbook results
- Stream progress updates

**Testing MCP:**

1. **Configure Claude Desktop** to use the MCP server:
```json
// Claude Desktop config (~/.claude/config.json)
{
  "mcpServers": {
    "octave-clone": {
      "url": "http://localhost:7777/mcp"
    }
  }
}
```

2. **Test from Claude Desktop:**
```
User: Generate a sales playbook for Octave selling to Sendoso

Claude (using MCP): *calls octave-clone MCP server*
- Creates new session
- Runs phase1_2_3_4_workflow
- Returns playbook results
```

**Tasks:**
- [ ] Verify MCP server is enabled (`enable_mcp_server=True`)
- [ ] Test MCP server endpoint (`http://localhost:7777/mcp`)
- [ ] Configure Claude Desktop to use MCP
- [ ] Test workflow execution via Claude Desktop
- [ ] Document MCP integration steps
- [ ] Create MCP usage examples

**Deliverables:**
- Working MCP server at `/mcp`
- Claude Desktop configuration guide
- MCP usage documentation

---

### 4.5.2 - Quality Assurance & Edge Case Testing
**Goal:** Comprehensively test the workflow with diverse scenarios

#### Test Suite Design

**1. Happy Path Tests** (Already validated)
- ‚úÖ Octave ‚Üí Sendoso (Phase 4 test)
- [ ] Test with 5+ different vendor/prospect pairs
  - SaaS ‚Üí SaaS (e.g., HubSpot ‚Üí Salesforce)
  - B2B Tech ‚Üí Enterprise (e.g., Snowflake ‚Üí JPMorgan)
  - Small vendor ‚Üí Large prospect
  - New vendor (sparse content) ‚Üí Established prospect

**2. Edge Case Tests**

**Test 2.1: Invalid Domains**
```python
Input: {
    "vendor_domain": "https://thisisnotarealwebsite12345.com",
    "prospect_domain": "https://sendoso.com"
}

Expected Behavior:
- Step 1 (validate_vendor_domain) should fail
- Workflow should stop with clear error message
- No wasted API calls to subsequent steps
```

**Test 2.2: Minimal Content Websites**
```python
Input: {
    "vendor_domain": "https://example.com",  # Single page, no product info
    "prospect_domain": "https://sendoso.com"
}

Expected Behavior:
- Step 2-3 succeed (scrape + analyze minimal content)
- Step 4 (prioritize_urls) finds few/no additional URLs
- Step 5 (batch_scrape) handles empty URL list
- Step 6 (vendor extraction) returns sparse data with warnings
- Playbook still generated, but flagged as "low confidence"
```

**Test 2.3: Non-English Websites**
```python
Input: {
    "vendor_domain": "https://www.rakuten.co.jp",  # Japanese
    "prospect_domain": "https://sendoso.com"
}

Expected Behavior:
- Firecrawl scrapes successfully
- GPT-4o handles multi-language content
- Playbook generated in English (or flag as multi-lingual)
```

**Test 2.4: Rate Limit Handling**
```python
# Simulate running 10 workflows in parallel
# (Could trigger Firecrawl/OpenAI rate limits)

Expected Behavior:
- Graceful retry with exponential backoff
- Clear error messages if rate limit persists
- Partial results saved before failure
```

**Test 2.5: Network Timeout**
```python
# Simulate slow/unresponsive website

Input: {
    "vendor_domain": "https://veryslow.website.com",
    "prospect_domain": "https://sendoso.com"
}

Expected Behavior:
- Firecrawl timeout after 60s (configurable)
- Workflow fails fast with clear error
- No hanging requests
```

**Test 2.6: Malformed JSON Responses**
```python
# Simulate agent returning invalid JSON

Expected Behavior:
- Pydantic validation catches schema errors
- Step fails with validation error message
- Workflow stops (fail fast)
```

**Test 2.7: Missing API Keys**
```python
# Run with missing OPENAI_API_KEY or FIRECRAWL_API_KEY

Expected Behavior:
- Step fails immediately with clear error: "OPENAI_API_KEY not found"
- Workflow stops before wasting API calls
```

**Test 2.8: Large Playbooks** (Token Limits)
```python
# Test with content-heavy vendors (100+ pages)

Input: {
    "vendor_domain": "https://aws.amazon.com",  # Massive site
    "prospect_domain": "https://sendoso.com"
}

Expected Behavior:
- Step 4 prioritizes top N pages (limit to 20-30)
- Step 5 batch scrape handles large content volumes
- Agents don't exceed token limits (truncate if needed)
```

**Test 2.9: Duplicate Runs**
```python
# Run same vendor/prospect pair twice

Expected Behavior:
- Both runs succeed independently
- Results may differ slightly (LLM non-determinism)
- Consider caching strategy for Phase 5
```

**Test 2.10: Concurrent Workflow Runs**
```python
# Run 5 workflows simultaneously via API

Expected Behavior:
- All workflows complete successfully
- No race conditions or shared state issues
- Performance metrics logged
```

#### Test Implementation

Create `tests/test_edge_cases.py`:

```python
"""
Edge case tests for Octave Clone workflow
Run with: pytest tests/test_edge_cases.py -v
"""

import pytest
from workflow import phase1_2_3_4_workflow

class TestEdgeCases:

    def test_invalid_vendor_domain(self):
        """Test that invalid domains fail gracefully"""
        result = phase1_2_3_4_workflow.run(input={
            "vendor_domain": "https://notarealsite12345.com",
            "prospect_domain": "https://sendoso.com"
        })

        assert result.success == False
        assert "domain" in result.content.lower()

    def test_minimal_content_website(self):
        """Test handling of minimal content sites"""
        result = phase1_2_3_4_workflow.run(input={
            "vendor_domain": "https://example.com",
            "prospect_domain": "https://sendoso.com"
        })

        # Should complete but with warnings
        assert result.success == True
        assert "low_confidence" in result.content.get("flags", [])

    def test_non_english_content(self):
        """Test multi-language content handling"""
        result = phase1_2_3_4_workflow.run(input={
            "vendor_domain": "https://www.rakuten.co.jp",
            "prospect_domain": "https://sendoso.com"
        })

        assert result.success == True
        # Playbook should still be in English
        playbook = result.content.get("sales_playbook", {})
        assert playbook.get("language") in ["en", "multi"]

    # ... more tests
```

**Tasks:**
- [ ] Create `tests/` directory
- [ ] Implement 10+ edge case tests
- [ ] Create test runner script (`run_tests.sh`)
- [ ] Document test results
- [ ] Fix bugs discovered during testing
- [ ] Create test coverage report

**Deliverables:**
- `tests/test_edge_cases.py` - Comprehensive test suite
- Test results report
- Bug fixes for discovered issues

---

### 4.5.3 - Performance Benchmarking & Optimization
**Goal:** Establish baseline metrics and optimize bottlenecks

#### Performance Metrics to Track

**Workflow-Level Metrics:**
- Total execution time (target: <3 minutes)
- Time per phase:
  - Phase 1 (Intelligence Gathering): ~60-90s
  - Phase 2 (Vendor Extraction): ~30-45s
  - Phase 3 (Prospect Analysis): ~20-30s
  - Phase 4 (Playbook Generation): ~30-45s

**Step-Level Metrics:**
- Step 1 (Domain Validation): <5s
- Step 2 (Homepage Scraping): 10-20s
- Step 3 (Homepage Analysis): 10-15s
- Step 4 (URL Prioritization): 5-10s
- Step 5 (Batch Scraping): 30-60s (depends on # URLs)
- Step 6 (Vendor Extraction): 20-30s (8 parallel agents)
- Step 7 (Prospect Analysis): 15-25s (3 agents)
- Step 8 (Playbook Generation): 25-35s (4 agents)

**API Call Metrics:**
- Firecrawl API calls per workflow: ~10-30
- OpenAI API calls per workflow: ~15-25
- Total tokens consumed: ~500k-1M tokens
- Estimated cost per playbook: $1-3

#### Benchmarking Script

Create `benchmark.py`:

```python
"""
Performance benchmarking for Octave Clone workflow
Runs multiple test cases and measures performance.
"""

from workflow import phase1_2_3_4_workflow
from datetime import datetime
import json

test_cases = [
    {
        "name": "Octave ‚Üí Sendoso",
        "vendor_domain": "https://octavehq.com",
        "prospect_domain": "https://sendoso.com"
    },
    {
        "name": "HubSpot ‚Üí Salesforce",
        "vendor_domain": "https://hubspot.com",
        "prospect_domain": "https://salesforce.com"
    },
    {
        "name": "Snowflake ‚Üí Databricks",
        "vendor_domain": "https://snowflake.com",
        "prospect_domain": "https://databricks.com"
    },
]

results = []

for test_case in test_cases:
    print(f"\n{'='*80}")
    print(f"Benchmarking: {test_case['name']}")
    print(f"{'='*80}")

    start = datetime.now()
    result = phase1_2_3_4_workflow.run(input={
        "vendor_domain": test_case["vendor_domain"],
        "prospect_domain": test_case["prospect_domain"]
    })
    end = datetime.now()
    duration = (end - start).total_seconds()

    results.append({
        "test_case": test_case["name"],
        "duration_seconds": duration,
        "success": result.success,
        "vendor_domain": test_case["vendor_domain"],
        "prospect_domain": test_case["prospect_domain"]
    })

    print(f"‚úÖ Completed in {duration:.1f}s")

# Save results
with open("benchmark_results.json", "w") as f:
    json.dump(results, f, indent=2)

# Print summary
print("\n" + "="*80)
print("BENCHMARK SUMMARY")
print("="*80)
for r in results:
    status = "‚úÖ" if r["success"] else "‚ùå"
    print(f"{status} {r['test_case']}: {r['duration_seconds']:.1f}s")

avg_duration = sum(r["duration_seconds"] for r in results) / len(results)
print(f"\nüìä Average Duration: {avg_duration:.1f}s")
```

#### Optimization Opportunities

**1. Parallel Step Optimization**
- Current: 8 parallel agents in Step 6, 3 in Step 8
- Opportunity: Profile to ensure parallelism is working
- Tool: Add timing logs to each parallel step

**2. Firecrawl Caching**
- Current: No caching between runs
- Opportunity: Cache homepage scrapes for 24h
- Savings: ~20-30s on repeated vendor/prospect pairs

**3. Prompt Optimization**
- Current: Large prompts sent to GPT-4o
- Opportunity: Reduce prompt size, use GPT-4o-mini for simple tasks
- Savings: ~30-50% cost reduction

**4. Batch Size Tuning**
- Current: Step 5 scrapes 10-20 URLs
- Opportunity: Test optimal batch size (5? 15? 25?)
- Trade-off: More URLs = better data, but slower execution

**Tasks:**
- [ ] Create `benchmark.py` script
- [ ] Run benchmarks on 5+ test cases
- [ ] Profile step-by-step execution times
- [ ] Identify bottlenecks (slowest steps)
- [ ] Implement caching for Firecrawl responses
- [ ] Test GPT-4o-mini for simple extraction tasks
- [ ] Optimize batch size in Step 5
- [ ] Re-run benchmarks after optimizations

**Deliverables:**
- `benchmark.py` - Benchmarking script
- `benchmark_results.json` - Performance metrics
- Optimization recommendations document
- Implemented optimizations

---

### 4.5.4 - Error Handling & Retry Logic
**Goal:** Implement robust failure recovery

#### Current Error Handling

**Fail Fast Pattern** (from workflow_helpers.py):
```python
def extract_validated_urls_or_fail(step_input: StepInput) -> dict:
    """Extract and validate URLs from previous step - fail fast if missing"""
    try:
        vendor_data = step_input.get_step_content("prioritize_urls")

        if not vendor_data or "vendor_urls" not in vendor_data:
            error_msg = "‚ùå URL prioritization failed - cannot proceed"
            print(f"\n{error_msg}")
            return {"error": error_msg, "should_stop": True}

        return vendor_data
    except Exception as e:
        error_msg = f"‚ùå Failed to extract URLs: {str(e)}"
        print(f"\n{error_msg}")
        return {"error": error_msg, "should_stop": True}
```

**Good:** Fails fast with clear errors
**Missing:** Retry logic for transient failures

#### Enhanced Error Handling

**Add Retry Logic for Transient Failures:**

Create `utilities/retry_helpers.py`:

```python
"""
Retry helpers for transient failures (rate limits, timeouts, network errors)
"""

import time
from functools import wraps
from typing import Callable, Any

def retry_with_backoff(
    max_retries: int = 3,
    initial_delay: float = 1.0,
    backoff_factor: float = 2.0,
    exceptions: tuple = (Exception,)
):
    """
    Retry decorator with exponential backoff

    Args:
        max_retries: Maximum number of retry attempts
        initial_delay: Initial delay in seconds
        backoff_factor: Multiplier for delay after each retry
        exceptions: Tuple of exceptions to catch and retry
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            delay = initial_delay
            last_exception = None

            for attempt in range(max_retries + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e

                    if attempt == max_retries:
                        print(f"\n‚ùå Max retries ({max_retries}) exceeded for {func.__name__}")
                        raise

                    print(f"\n‚ö†Ô∏è  {func.__name__} failed (attempt {attempt + 1}/{max_retries}): {str(e)}")
                    print(f"   Retrying in {delay:.1f}s...")
                    time.sleep(delay)
                    delay *= backoff_factor

            raise last_exception

        return wrapper
    return decorator


# Example usage in step functions:

@retry_with_backoff(max_retries=3, initial_delay=2.0)
def scrape_with_retry(url: str):
    """Scrape URL with automatic retry on failure"""
    from agno.tools.firecrawl import FirecrawlTools

    firecrawl = FirecrawlTools(scrape=True, crawl=False)
    result = firecrawl.scrape_url(url=url)

    if not result or "error" in result:
        raise Exception(f"Firecrawl failed: {result.get('error', 'Unknown error')}")

    return result
```

**Apply Retry Logic to Critical Steps:**

```python
# In steps/step2_homepage_scraping.py

from utilities.retry_helpers import retry_with_backoff

@retry_with_backoff(max_retries=3, initial_delay=2.0)
def scrape_vendor_homepage(step_input: StepInput) -> StepOutput:
    """Step 2a: Scrape vendor homepage with retry logic"""
    # ... existing code ...
```

**Graceful Degradation for Non-Critical Failures:**

Some failures shouldn't stop the workflow:
- Missing vendor case studies ‚Üí Continue with available data
- Firecrawl timeout on 1 URL ‚Üí Skip and continue with other URLs
- One persona extraction fails ‚Üí Continue with 2/3 personas

```python
def extract_case_studies(step_input: StepInput) -> StepOutput:
    """Extract case studies - gracefully degrade if none found"""
    try:
        # ... extraction logic ...

        if not case_studies:
            print("‚ö†Ô∏è  No case studies found - continuing with empty list")
            return StepOutput(
                content={"case_studies": [], "warning": "No case studies found"},
                success=True  # Still succeed, just with empty data
            )

    except Exception as e:
        # Log error but don't stop workflow
        print(f"‚ö†Ô∏è  Case study extraction failed: {str(e)}")
        return StepOutput(
            content={"case_studies": [], "error": str(e)},
            success=True  # Graceful degradation
        )
```

**Tasks:**
- [ ] Create `utilities/retry_helpers.py`
- [ ] Add retry logic to Firecrawl calls (Steps 2, 5)
- [ ] Add retry logic to agent calls (Steps 6, 7, 8)
- [ ] Implement graceful degradation for non-critical failures
- [ ] Test retry behavior with simulated failures
- [ ] Add retry metrics to logging
- [ ] Document retry strategy

**Deliverables:**
- `utilities/retry_helpers.py` - Retry helper functions
- Updated step functions with retry logic
- Graceful degradation implementation
- Retry testing results

---

### 4.5.5 - Logging & Monitoring
**Goal:** Production-grade observability

#### Structured Logging

**Current Logging:**
- Basic print statements
- No log levels
- No structured format
- Hard to parse/analyze

**Target Logging:**
- Structured JSON logs
- Log levels (DEBUG, INFO, WARNING, ERROR)
- Contextual metadata (workflow_id, step_name, duration)
- Easy to parse and search

**Implementation:**

Create `utilities/logging_config.py`:

```python
"""
Structured logging configuration for Octave Clone
"""

import logging
import json
import sys
from datetime import datetime
from typing import Any

class StructuredFormatter(logging.Formatter):
    """
    JSON formatter for structured logging
    """

    def format(self, record: logging.LogRecord) -> str:
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno,
        }

        # Add extra fields if present
        if hasattr(record, "workflow_id"):
            log_data["workflow_id"] = record.workflow_id
        if hasattr(record, "step_name"):
            log_data["step_name"] = record.step_name
        if hasattr(record, "duration_ms"):
            log_data["duration_ms"] = record.duration_ms

        # Add exception info if present
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)

        return json.dumps(log_data)


def setup_logging(level: str = "INFO") -> logging.Logger:
    """
    Setup structured logging for Octave Clone

    Args:
        level: Log level (DEBUG, INFO, WARNING, ERROR)

    Returns:
        Configured logger instance
    """
    logger = logging.getLogger("octave_clone")
    logger.setLevel(getattr(logging, level.upper()))

    # Console handler with structured formatting
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(StructuredFormatter())
    logger.addHandler(handler)

    return logger


# Global logger instance
logger = setup_logging()


# Helper functions for common log patterns

def log_step_start(step_name: str, workflow_id: str = None):
    """Log step start event"""
    logger.info(
        f"Starting step: {step_name}",
        extra={"step_name": step_name, "workflow_id": workflow_id, "event": "step_start"}
    )


def log_step_end(step_name: str, duration_ms: float, workflow_id: str = None, success: bool = True):
    """Log step completion event"""
    logger.info(
        f"Completed step: {step_name} in {duration_ms:.0f}ms",
        extra={
            "step_name": step_name,
            "workflow_id": workflow_id,
            "duration_ms": duration_ms,
            "success": success,
            "event": "step_end"
        }
    )


def log_error(message: str, error: Exception, step_name: str = None, workflow_id: str = None):
    """Log error event"""
    logger.error(
        message,
        exc_info=error,
        extra={"step_name": step_name, "workflow_id": workflow_id, "event": "error"}
    )
```

**Usage in Step Functions:**

```python
from utilities.logging_config import logger, log_step_start, log_step_end, log_error
from datetime import datetime

def scrape_vendor_homepage(step_input: StepInput) -> StepOutput:
    """Step 2a: Scrape vendor homepage"""
    step_name = "scrape_vendor_homepage"
    workflow_id = step_input.workflow_run_id  # Get from AgentOS

    log_step_start(step_name, workflow_id)
    start_time = datetime.now()

    try:
        # ... step logic ...

        duration_ms = (datetime.now() - start_time).total_seconds() * 1000
        log_step_end(step_name, duration_ms, workflow_id, success=True)

        return StepOutput(content=result, success=True)

    except Exception as e:
        duration_ms = (datetime.now() - start_time).total_seconds() * 1000
        log_error(f"Step {step_name} failed", e, step_name, workflow_id)
        log_step_end(step_name, duration_ms, workflow_id, success=False)

        return StepOutput(content={"error": str(e)}, success=False, stop=True)
```

**Example Log Output:**

```json
{"timestamp": "2025-11-02T10:30:00.123Z", "level": "INFO", "logger": "octave_clone", "message": "Starting step: scrape_vendor_homepage", "module": "step2_homepage_scraping", "function": "scrape_vendor_homepage", "line": 45, "step_name": "scrape_vendor_homepage", "workflow_id": "run_abc123", "event": "step_start"}

{"timestamp": "2025-11-02T10:30:15.456Z", "level": "INFO", "logger": "octave_clone", "message": "Completed step: scrape_vendor_homepage in 15333ms", "module": "step2_homepage_scraping", "function": "scrape_vendor_homepage", "line": 67, "step_name": "scrape_vendor_homepage", "workflow_id": "run_abc123", "duration_ms": 15333.0, "success": true, "event": "step_end"}
```

**Tasks:**
- [ ] Create `utilities/logging_config.py`
- [ ] Add structured logging to all step functions
- [ ] Log workflow start/end events
- [ ] Log API call metrics (tokens, cost)
- [ ] Create log analysis script (`analyze_logs.py`)
- [ ] Test log output format
- [ ] Document logging conventions

**Deliverables:**
- `utilities/logging_config.py` - Logging configuration
- Updated step functions with structured logging
- Log analysis tooling
- Logging documentation

---

### 4.5.6 - Session Management & Run History
**Goal:** Track workflow runs and enable result retrieval

With AgentOS, we get session management for free! But we should leverage it effectively.

**Session Features:**
- **Session ID**: Unique identifier for each workflow run
- **Session Persistence**: Results stored in memory (or DB if configured)
- **Session Retrieval**: Fetch results of past runs
- **Session Deletion**: Clean up old runs

**API Endpoints** (provided by AgentOS):

```bash
# Create new run (new session)
curl -X POST http://localhost:7777/workflows/phase1_2_3_4_workflow/runs \
    -d 'vendor_domain=https://octavehq.com' \
    -d 'prospect_domain=https://sendoso.com'

Response:
{
  "session_id": "sess_abc123",
  "run_id": "run_xyz789",
  "status": "running"
}

# Get run status
curl http://localhost:7777/workflows/phase1_2_3_4_workflow/runs/run_xyz789

# Get session history
curl http://localhost:7777/sessions/sess_abc123

# Delete session
curl -X DELETE http://localhost:7777/sessions/sess_abc123
```

**Tasks:**
- [ ] Test session creation via API
- [ ] Test session retrieval
- [ ] Test session deletion
- [ ] Create session management examples
- [ ] Document session workflow
- [ ] Add session IDs to logging

**Deliverables:**
- Session management documentation
- Example API calls
- Integration with logging

---

### 4.5.7 - Documentation & Deployment Guide
**Goal:** Comprehensive documentation for AgentOS deployment

**Documentation Sections:**

**1. Quick Start Guide**
```markdown
# Octave Clone - Quick Start

## Prerequisites
- Python 3.9+
- API keys: OPENAI_API_KEY, FIRECRAWL_API_KEY, ANTHROPIC_API_KEY

## Installation
pip install -U agno "fastapi[standard]" uvicorn openai anthropic

## Run Locally
python my_octave_os.py

## Test API
curl -X POST http://localhost:7777/workflows/phase1_2_3_4_workflow/runs \
    -d 'vendor_domain=https://octavehq.com' \
    -d 'prospect_domain=https://sendoso.com'
```

**2. API Reference**
- All endpoints with examples
- Request/response schemas
- Error codes
- Rate limiting

**3. MCP Integration Guide**
- Claude Desktop setup
- Example prompts
- Troubleshooting

**4. Deployment Guide**
- Local deployment (Phase 4.5)
- Cloud deployment prep (Phase 5)
- Environment variables
- Database setup (optional)

**Tasks:**
- [ ] Create `docs/` directory
- [ ] Write Quick Start guide
- [ ] Write API reference
- [ ] Write MCP integration guide
- [ ] Write deployment guide
- [ ] Create troubleshooting FAQ
- [ ] Add code examples

**Deliverables:**
- `docs/quick-start.md`
- `docs/api-reference.md`
- `docs/mcp-integration.md`
- `docs/deployment.md`
- `docs/troubleshooting.md`

---

## üöÄ Phase 4.5 Timeline

**Week 1: AgentOS & Testing**
- Day 1-2: AgentOS setup + MCP integration (4.5.1)
- Day 3-4: Edge case testing (4.5.2)
- Day 5: Performance benchmarking (4.5.3)

**Week 2: Hardening & Documentation**
- Day 1-2: Error handling + retry logic (4.5.4)
- Day 3: Structured logging (4.5.5)
- Day 4: Session management (4.5.6)
- Day 5: Documentation (4.5.7)

**Total Duration:** 1-2 weeks

---

## üéØ Success Criteria

**Phase 4.5 is complete when:**

‚úÖ **AgentOS Running:** `my_octave_os.py` serves API at `http://localhost:7777`
‚úÖ **MCP Enabled:** Claude Desktop can run workflows via MCP
‚úÖ **Edge Cases Tested:** 10+ edge case tests pass
‚úÖ **Performance Benchmarked:** <3 min average execution time
‚úÖ **Error Handling:** Retry logic for transient failures
‚úÖ **Logging:** Structured JSON logs for all steps
‚úÖ **Documentation:** Complete API reference and deployment guide
‚úÖ **Production Ready:** Blueprint for Phase 5 deployment

---

## üìä Deliverables Summary

**Code:**
- `my_octave_os.py` - AgentOS instance with workflow + MCP
- `tests/test_edge_cases.py` - Edge case test suite
- `benchmark.py` - Performance benchmarking script
- `utilities/retry_helpers.py` - Retry logic
- `utilities/logging_config.py` - Structured logging

**Documentation:**
- `docs/quick-start.md`
- `docs/api-reference.md`
- `docs/mcp-integration.md`
- `docs/deployment.md`
- `docs/troubleshooting.md`

**Reports:**
- `test_results.md` - Edge case test results
- `benchmark_results.json` - Performance metrics
- `optimization_recommendations.md` - Optimization findings

---

## üîß Technology Stack

**Phase 4.5 Additions:**
- **AgentOS** - FastAPI-based agentic OS
- **MCP** - Model Context Protocol for Claude integration
- **pytest** - Testing framework
- **Logging** - Python `logging` with JSON formatter

**Existing Stack:**
- Agno (Workflow orchestration)
- Firecrawl (Web scraping)
- OpenAI GPT-4o (LLM agents)
- Anthropic Claude (Alternative LLM)

---

## üéâ Phase 4.5 Vision

By the end of Phase 4.5, Octave Clone will be:

‚úÖ **API-Accessible:** RESTful API for programmatic access
‚úÖ **Claude-Integrated:** Usable directly from Claude Desktop via MCP
‚úÖ **Battle-Tested:** 10+ edge cases validated
‚úÖ **Observable:** Structured logging for debugging
‚úÖ **Robust:** Retry logic for transient failures
‚úÖ **Documented:** Complete guides for deployment and usage
‚úÖ **Phase 5 Ready:** Clear blueprint for multi-tenant SaaS

**The result:** A production-grade API service that's ready to scale to multi-tenant SaaS in Phase 5.

---

**Ready to begin Phase 4.5?** Let's ship a rock-solid foundation! üöÄ
